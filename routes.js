import express from 'express';
import fetch from 'node-fetch';
import { getConfig, getModelById, getEndpointByType, getSystemPrompt, getModelReasoning } from './config.js';
import { logInfo, logDebug, logError, logRequest, logResponse } from './logger.js';
import { transformToAnthropic, getAnthropicHeaders } from './transformers/request-anthropic.js';
import { transformToOpenAI, getOpenAIHeaders } from './transformers/request-openai.js';
import { transformToCommon, getCommonHeaders } from './transformers/request-common.js';
import { AnthropicResponseTransformer } from './transformers/response-anthropic.js';
import { OpenAIResponseTransformer } from './transformers/response-openai.js';
import { getApiKey } from './auth.js';
import keyPoolManager from './auth.js';
import fetchWithPool from './utils/http-client.js';
import {
  getNextKeyFromPool,
  handle402Error,
  handleUpstreamError,
  handleStreamResponse,
  handleNonStreamResponse,
  recordTokenUsage
} from './utils/route-helpers.js';
import {
  createTokenStats,
  extractAnthropicTokens,
  extractOpenAITokens,
  extractCommonTokens
} from './utils/token-extractor.js';
const router = express.Router();



/**
 * Convert a /v1/responses API result to a /v1/chat/completions-compatible format.
 * Works for non-streaming responses.
 */
function convertResponseToChatCompletion(resp) {
  if (!resp || typeof resp !== 'object') {
    throw new Error('Invalid response object');
  }

  const outputMsg = (resp.output || []).find(o => o.type === 'message');
  const textBlocks = outputMsg?.content?.filter(c => c.type === 'output_text') || [];
  const content = textBlocks.map(c => c.text).join('');

  const chatCompletion = {
    id: resp.id ? resp.id.replace(/^resp_/, 'chatcmpl-') : `chatcmpl-${Date.now()}`,
    object: 'chat.completion',
    created: resp.created_at || Math.floor(Date.now() / 1000),
    model: resp.model || 'unknown-model',
    choices: [
      {
        index: 0,
        message: {
          role: outputMsg?.role || 'assistant',
          content: content || ''
        },
        finish_reason: resp.status === 'completed' ? 'stop' : 'unknown'
      }
    ],
    usage: {
      prompt_tokens: resp.usage?.input_tokens ?? 0,
      completion_tokens: resp.usage?.output_tokens ?? 0,
      total_tokens: resp.usage?.total_tokens ?? 0
    }
  };

  return chatCompletion;
}

router.get('/v1/models', (req, res) => {
  logInfo('GET /v1/models');
  
  try {
    const config = getConfig();
    const models = config.models.map(model => ({
      id: model.id,
      object: 'model',
      created: Date.now(),
      owned_by: model.type,
      permission: [],
      root: model.id,
      parent: null
    }));

    const response = {
      object: 'list',
      data: models
    };

    logResponse(200, null, response);
    res.json(response);
  } catch (error) {
    logError('Error in GET /v1/models', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

// æ ‡å‡† OpenAI èŠå¤©è¡¥å…¨å¤„ç†å‡½æ•°ï¼ˆå¸¦æ ¼å¼è½¬æ¢ï¼‰
async function handleChatCompletions(req, res) {
  logInfo('POST /v1/chat/completions');
  
  try {
    const openaiRequest = req.body;
    const modelId = openaiRequest.model;

    if (!modelId) {
      return res.status(400).json({ error: 'model is required' });
    }

    const model = getModelById(modelId);
    if (!model) {
      return res.status(404).json({ error: `Model ${modelId} not found` });
    }

    const endpoint = getEndpointByType(model.type);
    if (!endpoint) {
      return res.status(500).json({ error: `Endpoint type ${model.type} not found` });
    }

    logInfo(`Routing to ${model.type} endpoint: ${endpoint.base_url}`);

    // Get API key from pool (è½®è¯¢è·å–)
    let authHeader;
    let currentKeyId;
    try {
      const keyResult = await keyPoolManager.getNextKey();
      authHeader = `Bearer ${keyResult.key}`;
      currentKeyId = keyResult.keyId;
    } catch (error) {
      logError('Failed to get API key from pool', error);
      return res.status(500).json({
        error: 'å¯†é’¥æ± é”™è¯¯',
        message: error.message
      });
    }

    let transformedRequest;
    let headers;
    const clientHeaders = req.headers;

    // Log received client headers for debugging
    logDebug('Client headers received', {
      'x-factory-client': clientHeaders['x-factory-client'],
      'x-session-id': clientHeaders['x-session-id'],
      'x-assistant-message-id': clientHeaders['x-assistant-message-id'],
      'user-agent': clientHeaders['user-agent']
    });

    if (model.type === 'anthropic') {
      transformedRequest = transformToAnthropic(openaiRequest);
      const isStreaming = openaiRequest.stream === true;
      headers = getAnthropicHeaders(authHeader, clientHeaders, isStreaming, modelId);
    } else if (model.type === 'openai') {
      transformedRequest = transformToOpenAI(openaiRequest);
      headers = getOpenAIHeaders(authHeader, clientHeaders);
    } else if (model.type === 'common') {
      transformedRequest = transformToCommon(openaiRequest);
      headers = getCommonHeaders(authHeader, clientHeaders);
    } else {
      return res.status(500).json({ error: `Unknown endpoint type: ${model.type}` });
    }

    logRequest('POST', endpoint.base_url, headers, transformedRequest);

    // BaSuiï¼šğŸš€ ä½¿ç”¨ HTTP è¿æ¥æ± ï¼ˆå¤ç”¨ TCP è¿æ¥ï¼Œå‡å°‘æ¡æ‰‹å¼€é”€ï¼‰
    const response = await fetchWithPool(endpoint.base_url, {
      method: 'POST',
      headers,
      body: JSON.stringify(transformedRequest)
    });

    logInfo(`Response status: ${response.status}`);

    // å¤„ç†402é”™è¯¯ - æ°¸ä¹…å°ç¦å¯†é’¥
    if (response.status === 402) {
      keyPoolManager.banKey(currentKeyId, 'Payment Required - No Credits');
      const errorText = await response.text();
      logError(`Key banned due to 402 error: ${currentKeyId}`, new Error(errorText));
      return res.status(402).json({
        error: 'Payment Required',
        message: 'Key has been banned due to insufficient credits',
        details: errorText
      });
    }

    if (!response.ok) {
      const errorText = await response.text();
      logError(`Endpoint error: ${response.status}`, new Error(errorText));
      return res.status(response.status).json({
        error: `Endpoint returned ${response.status}`,
        details: errorText
      });
    }

    const isStreaming = transformedRequest.stream === true;

    if (isStreaming) {
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');

      // BaSui: æ”¶é›†æµå¼å“åº”ä¸­çš„Tokenç»Ÿè®¡ï¼ˆå®Œæ•´ç‰ˆ - åŒ…å« thinking å’Œ cache tokensï¼‰
      const tokenStats = createTokenStats();

      // common ç±»å‹ç›´æ¥è½¬å‘ï¼Œä¸ä½¿ç”¨ transformer
      if (model.type === 'common') {
        try {
          let buffer = '';
          for await (const chunk of response.body) {
            res.write(chunk);

            // BaSui: è§£æSSEæµï¼Œæå–Tokenä½¿ç”¨é‡
            buffer += chunk.toString();
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            for (const line of lines) {
              if (line.startsWith('data:')) {
                try {
                  const dataStr = line.slice(5).trim();
                  if (dataStr === '[DONE]') continue;
                  const data = JSON.parse(dataStr);
                  // ä½¿ç”¨æ–°çš„ Token æå–å·¥å…·å‡½æ•°
                  extractCommonTokens(data, tokenStats);
                } catch (e) {
                  // å¿½ç•¥éJSONè¡Œ
                }
              }
            }
          }
          res.end();
          logInfo('Stream forwarded (common type)');
        } catch (streamError) {
          logError('Stream error', streamError);
          res.end();
        }
      } else {
        // anthropic å’Œ openai ç±»å‹ä½¿ç”¨ transformer
        let transformer;
        let buffer = '';

        if (model.type === 'anthropic') {
          transformer = new AnthropicResponseTransformer(modelId, `chatcmpl-${Date.now()}`);
        } else if (model.type === 'openai') {
          transformer = new OpenAIResponseTransformer(modelId, `chatcmpl-${Date.now()}`);
        }

        try {
          // BaSui: åŒæ—¶è§£æåŸå§‹æµå’Œè½¬æ¢åçš„æµ
          const rawChunks = [];
          for await (const chunk of response.body) {
            rawChunks.push(chunk);
          }

          // BaSui: å…ˆä»åŸå§‹æµä¸­æå–Tokenç»Ÿè®¡
          for (const chunk of rawChunks) {
            buffer += chunk.toString();
            const lines = buffer.split('\n');
            buffer = lines.pop() || '';

            for (const line of lines) {
              if (line.startsWith('data:')) {
                try {
                  const dataStr = line.slice(5).trim();
                  if (dataStr === '[DONE]') continue;

                  const data = JSON.parse(dataStr);

                  if (model.type === 'anthropic') {
                    // ä½¿ç”¨æ–°çš„ Token æå–å·¥å…·å‡½æ•°ï¼ˆè‡ªåŠ¨å¤„ç†æ‰€æœ‰ Token ç±»å‹ï¼‰
                    extractAnthropicTokens(data, tokenStats);
                  } else {
                    // ä½¿ç”¨æ–°çš„ Token æå–å·¥å…·å‡½æ•°ï¼ˆOpenAI æ ¼å¼ï¼‰
                    extractOpenAITokens(data, tokenStats);
                  }
                } catch (e) {
                  // å¿½ç•¥éJSONè¡Œ
                }
              }
            }
          }

          // BaSui: ç„¶åè½¬æ¢å¹¶è½¬å‘
          const rawStream = (async function* () {
            for (const chunk of rawChunks) {
              yield chunk;
            }
          })();

          for await (const chunk of transformer.transformStream(rawStream)) {
            res.write(chunk);
          }
          res.end();
          logInfo('Stream completed');
        } catch (streamError) {
          logError('Stream error', streamError);
          res.end();
        }
      }

      // BaSui: è®°å½•Tokenä½¿ç”¨é‡ç»Ÿè®¡ï¼ˆåŒ…å«å®Œæ•´çš„ Token ç±»å‹ï¼‰
      try {
        if (tokenStats.inputTokens > 0 || tokenStats.outputTokens > 0) {
          const { recordRequest } = await import('./utils/request-stats.js');
          recordRequest({
            inputTokens: tokenStats.inputTokens,
            outputTokens: tokenStats.outputTokens,
            thinkingTokens: tokenStats.thinkingTokens,
            cacheCreationTokens: tokenStats.cacheCreationTokens,
            cacheReadTokens: tokenStats.cacheReadTokens,
            model: modelId,
            success: true
          });
          logDebug(`æµå¼å“åº”Tokenç»Ÿè®¡(/v1/chat/completions): input=${tokenStats.inputTokens}, output=${tokenStats.outputTokens}, thinking=${tokenStats.thinkingTokens}, cache_creation=${tokenStats.cacheCreationTokens}, cache_read=${tokenStats.cacheReadTokens}`);
        }
      } catch (err) {
        logError('è®°å½•Tokenç»Ÿè®¡å¤±è´¥', err);
      }
    } else {
      const data = await response.json();

      // è®°å½•Tokenä½¿ç”¨é‡
      recordTokenUsage(data, model.type, currentKeyId);

      if (model.type === 'openai') {
        try {
          const converted = convertResponseToChatCompletion(data);
          logResponse(200, null, converted);
          res.json(converted);
        } catch (e) {
          // å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå›é€€ä¸ºåŸå§‹æ•°æ®
          logResponse(200, null, data);
          res.json(data);
        }
      } else {
        // anthropic/common: ä¿æŒç°æœ‰é€»è¾‘ï¼Œç›´æ¥è½¬å‘
        logResponse(200, null, data);
        res.json(data);
      }
    }

  } catch (error) {
    logError('Error in /v1/chat/completions', error);
    // BaSuiï¼šæ£€æŸ¥å“åº”æ˜¯å¦å·²ç»å¼€å§‹å‘é€ï¼Œé¿å…é‡å¤å‘é€å¯¼è‡´å´©æºƒ
    if (!res.headersSent) {
      res.status(500).json({
        error: 'Internal server error',
        message: error.message
      });
    } else {
      // æµå¼å“åº”å·²å¼€å§‹ï¼Œç›´æ¥ç»“æŸè¿æ¥
      res.end();
    }
  }
}

// ç›´æ¥è½¬å‘ OpenAI è¯·æ±‚ï¼ˆä¸åšæ ¼å¼è½¬æ¢ï¼‰
async function handleDirectResponses(req, res) {
  logInfo('POST /v1/responses');
  
  try {
    const openaiRequest = req.body;
    const modelId = openaiRequest.model;

    if (!modelId) {
      return res.status(400).json({ error: 'model is required' });
    }

    const model = getModelById(modelId);
    if (!model) {
      return res.status(404).json({ error: `Model ${modelId} not found` });
    }

    // åªå…è®¸ openai ç±»å‹ç«¯ç‚¹
    if (model.type !== 'openai') {
      return res.status(400).json({
        error: 'Invalid endpoint type',
        message: `/v1/responses æ¥å£åªæ”¯æŒ openai ç±»å‹ç«¯ç‚¹ï¼Œå½“å‰æ¨¡å‹ ${modelId} æ˜¯ ${model.type} ç±»å‹`
      });
    }

    const endpoint = getEndpointByType(model.type);
    if (!endpoint) {
      return res.status(500).json({ error: `Endpoint type ${model.type} not found` });
    }

    logInfo(`Direct forwarding to ${model.type} endpoint: ${endpoint.base_url}`);

    // Get API key from pool (è½®è¯¢è·å–)
    let authHeader;
    let currentKeyId;
    try {
      const keyResult = await keyPoolManager.getNextKey();
      authHeader = `Bearer ${keyResult.key}`;
      currentKeyId = keyResult.keyId;
    } catch (error) {
      logError('Failed to get API key from pool', error);
      return res.status(500).json({
        error: 'å¯†é’¥æ± é”™è¯¯',
        message: error.message
      });
    }

    const clientHeaders = req.headers;

    // è·å– headers
    const headers = getOpenAIHeaders(authHeader, clientHeaders);

    // æ³¨å…¥ç³»ç»Ÿæç¤ºåˆ° instructions å­—æ®µ
    const systemPrompt = getSystemPrompt();
    const modifiedRequest = { ...openaiRequest };
    if (systemPrompt) {
      // å¦‚æœå·²æœ‰ instructionsï¼Œåˆ™åœ¨å‰é¢æ·»åŠ ç³»ç»Ÿæç¤º
      if (modifiedRequest.instructions) {
        modifiedRequest.instructions = systemPrompt + modifiedRequest.instructions;
      } else {
        // å¦åˆ™ç›´æ¥è®¾ç½®ç³»ç»Ÿæç¤º
        modifiedRequest.instructions = systemPrompt;
      }
    }

    // å¤„ç†reasoningå­—æ®µ
    const reasoningLevel = getModelReasoning(modelId);
    if (reasoningLevel === 'auto') {
      // Autoæ¨¡å¼ï¼šä¿æŒåŸå§‹è¯·æ±‚çš„reasoningå­—æ®µä¸å˜
      // å¦‚æœåŸå§‹è¯·æ±‚æœ‰reasoningå­—æ®µå°±ä¿ç•™ï¼Œæ²¡æœ‰å°±ä¸æ·»åŠ 
    } else if (reasoningLevel && ['low', 'medium', 'high'].includes(reasoningLevel)) {
      modifiedRequest.reasoning = {
        effort: reasoningLevel,
        summary: 'auto'
      };
    } else {
      // å¦‚æœé…ç½®æ˜¯offæˆ–æ— æ•ˆï¼Œç§»é™¤reasoningå­—æ®µ
      delete modifiedRequest.reasoning;
    }

    logRequest('POST', endpoint.base_url, headers, modifiedRequest);

    // BaSuiï¼šğŸš€ ä½¿ç”¨ HTTP è¿æ¥æ± è½¬å‘ä¿®æ”¹åçš„è¯·æ±‚
    const response = await fetchWithPool(endpoint.base_url, {
      method: 'POST',
      headers,
      body: JSON.stringify(modifiedRequest)
    });

    logInfo(`Response status: ${response.status}`);

    // å¤„ç†402é”™è¯¯ - æ°¸ä¹…å°ç¦å¯†é’¥
    if (response.status === 402) {
      keyPoolManager.banKey(currentKeyId, 'Payment Required - No Credits');
      const errorText = await response.text();
      logError(`Key banned due to 402 error: ${currentKeyId}`, new Error(errorText));
      return res.status(402).json({
        error: 'Payment Required',
        message: 'Key has been banned due to insufficient credits',
        details: errorText
      });
    }

    if (!response.ok) {
      const errorText = await response.text();
      logError(`Endpoint error: ${response.status}`, new Error(errorText));
      return res.status(response.status).json({
        error: `Endpoint returned ${response.status}`,
        details: errorText
      });
    }

    const isStreaming = openaiRequest.stream === true;

    if (isStreaming) {
      // ç›´æ¥è½¬å‘æµå¼å“åº”ï¼Œä¸åšä»»ä½•è½¬æ¢
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');

      try {
        // BaSui: æ”¶é›†æµå¼å“åº”ä¸­çš„Tokenç»Ÿè®¡ï¼ˆå®Œæ•´ç‰ˆï¼‰
        const tokenStats = createTokenStats();
        let buffer = '';

        // ç›´æ¥å°†åŸå§‹å“åº”æµè½¬å‘ç»™å®¢æˆ·ç«¯
        for await (const chunk of response.body) {
          res.write(chunk);

          // BaSui: è§£æSSEæµï¼Œæå–Tokenä½¿ç”¨é‡ï¼ˆOpenAIæ ¼å¼ï¼‰
          buffer += chunk.toString();
          const lines = buffer.split('\n');
          buffer = lines.pop() || ''; // ä¿ç•™æœ€åä¸å®Œæ•´çš„è¡Œ

          for (const line of lines) {
            if (line.startsWith('data:')) {
              try {
                const dataStr = line.slice(5).trim();
                if (dataStr === '[DONE]') continue;

                const data = JSON.parse(dataStr);
                // ä½¿ç”¨æ–°çš„ Token æå–å·¥å…·å‡½æ•°ï¼ˆOpenAI æ ¼å¼ï¼‰
                extractOpenAITokens(data, tokenStats);
              } catch (e) {
                // å¿½ç•¥éJSONè¡Œ
              }
            }
          }
        }
        res.end();
        logInfo('Stream forwarded successfully');

        // BaSui: è®°å½•Tokenä½¿ç”¨é‡ç»Ÿè®¡ï¼ˆåŒ…å«å®Œæ•´çš„ Token ç±»å‹ï¼‰
        if (tokenStats.inputTokens > 0 || tokenStats.outputTokens > 0) {
          const { recordRequest } = await import('./utils/request-stats.js');
          recordRequest({
            inputTokens: tokenStats.inputTokens,
            outputTokens: tokenStats.outputTokens,
            thinkingTokens: tokenStats.thinkingTokens,
            cacheCreationTokens: tokenStats.cacheCreationTokens,
            cacheReadTokens: tokenStats.cacheReadTokens,
            model: modelId,
            success: true
          });
          logDebug(`æµå¼å“åº”Tokenç»Ÿè®¡(/v1/responses): input=${tokenStats.inputTokens}, output=${tokenStats.outputTokens}, thinking=${tokenStats.thinkingTokens}, cache_creation=${tokenStats.cacheCreationTokens}, cache_read=${tokenStats.cacheReadTokens}`);
        }
      } catch (streamError) {
        logError('Stream error', streamError);
        res.end();
      }
    } else {
      // ç›´æ¥è½¬å‘éæµå¼å“åº”ï¼Œä¸åšä»»ä½•è½¬æ¢
      const data = await response.json();

      // è®°å½•Tokenä½¿ç”¨é‡
      recordTokenUsage(data, 'openai', currentKeyId);

      logResponse(200, null, data);
      res.json(data);
    }

  } catch (error) {
    logError('Error in /v1/responses', error);
    // BaSuiï¼šæ£€æŸ¥å“åº”æ˜¯å¦å·²ç»å¼€å§‹å‘é€ï¼Œé¿å…é‡å¤å‘é€å¯¼è‡´å´©æºƒ
    if (!res.headersSent) {
      res.status(500).json({
        error: 'Internal server error',
        message: error.message
      });
    } else {
      // æµå¼å“åº”å·²å¼€å§‹ï¼Œç›´æ¥ç»“æŸè¿æ¥
      res.end();
    }
  }
}

// ç›´æ¥è½¬å‘ Anthropic è¯·æ±‚ï¼ˆä¸åšæ ¼å¼è½¬æ¢ï¼‰
async function handleDirectMessages(req, res) {
  logInfo('POST /v1/messages');
  
  try {
    const anthropicRequest = req.body;
    const modelId = anthropicRequest.model;

    if (!modelId) {
      return res.status(400).json({ error: 'model is required' });
    }

    const model = getModelById(modelId);
    if (!model) {
      return res.status(404).json({ error: `Model ${modelId} not found` });
    }

    // åªå…è®¸ anthropic ç±»å‹ç«¯ç‚¹
    if (model.type !== 'anthropic') {
      return res.status(400).json({ 
        error: 'Invalid endpoint type',
        message: `/v1/messages æ¥å£åªæ”¯æŒ anthropic ç±»å‹ç«¯ç‚¹ï¼Œå½“å‰æ¨¡å‹ ${modelId} æ˜¯ ${model.type} ç±»å‹`
      });
    }

    const endpoint = getEndpointByType(model.type);
    if (!endpoint) {
      return res.status(500).json({ error: `Endpoint type ${model.type} not found` });
    }

    logInfo(`Direct forwarding to ${model.type} endpoint: ${endpoint.base_url}`);

    // Get API key from pool (è½®è¯¢è·å–)
    let authHeader;
    let currentKeyId;
    try {
      const keyResult = await keyPoolManager.getNextKey();
      authHeader = `Bearer ${keyResult.key}`;
      currentKeyId = keyResult.keyId;
    } catch (error) {
      logError('Failed to get API key from pool', error);
      return res.status(500).json({
        error: 'å¯†é’¥æ± é”™è¯¯',
        message: error.message
      });
    }

    const clientHeaders = req.headers;

    // è·å– headers
    const isStreaming = anthropicRequest.stream === true;
    const headers = getAnthropicHeaders(authHeader, clientHeaders, isStreaming, modelId);

    // æ³¨å…¥ç³»ç»Ÿæç¤ºåˆ° system å­—æ®µ
    const systemPrompt = getSystemPrompt();
    const modifiedRequest = { ...anthropicRequest };
    if (systemPrompt) {
      if (modifiedRequest.system && Array.isArray(modifiedRequest.system)) {
        // å¦‚æœå·²æœ‰ system æ•°ç»„ï¼Œåˆ™åœ¨æœ€å‰é¢æ’å…¥ç³»ç»Ÿæç¤º
        modifiedRequest.system = [
          { type: 'text', text: systemPrompt },
          ...modifiedRequest.system
        ];
      } else {
        // å¦åˆ™åˆ›å»ºæ–°çš„ system æ•°ç»„
        modifiedRequest.system = [
          { type: 'text', text: systemPrompt }
        ];
      }
    }

    // å¤„ç†thinkingå­—æ®µ
    const reasoningLevel = getModelReasoning(modelId);
    if (reasoningLevel === 'auto') {
      // Autoæ¨¡å¼ï¼šä¿æŒåŸå§‹è¯·æ±‚çš„thinkingå­—æ®µä¸å˜
      // å¦‚æœåŸå§‹è¯·æ±‚æœ‰thinkingå­—æ®µå°±ä¿ç•™ï¼Œæ²¡æœ‰å°±ä¸æ·»åŠ 
    } else if (reasoningLevel && ['low', 'medium', 'high'].includes(reasoningLevel)) {
      const budgetTokens = {
        'low': 4096,
        'medium': 12288,
        'high': 24576
      };
      
      modifiedRequest.thinking = {
        type: 'enabled',
        budget_tokens: budgetTokens[reasoningLevel]
      };
    } else {
      // å¦‚æœé…ç½®æ˜¯offæˆ–æ— æ•ˆï¼Œç§»é™¤thinkingå­—æ®µ
      delete modifiedRequest.thinking;
    }

    logRequest('POST', endpoint.base_url, headers, modifiedRequest);

    // BaSuiï¼šğŸš€ ä½¿ç”¨ HTTP è¿æ¥æ± è½¬å‘ä¿®æ”¹åçš„è¯·æ±‚
    const response = await fetchWithPool(endpoint.base_url, {
      method: 'POST',
      headers,
      body: JSON.stringify(modifiedRequest)
    });

    logInfo(`Response status: ${response.status}`);

    // å¤„ç†402é”™è¯¯ - æ°¸ä¹…å°ç¦å¯†é’¥
    if (response.status === 402) {
      keyPoolManager.banKey(currentKeyId, 'Payment Required - No Credits');
      const errorText = await response.text();
      logError(`Key banned due to 402 error: ${currentKeyId}`, new Error(errorText));
      return res.status(402).json({
        error: 'Payment Required',
        message: 'Key has been banned due to insufficient credits',
        details: errorText
      });
    }

    if (!response.ok) {
      const errorText = await response.text();
      logError(`Endpoint error: ${response.status}`, new Error(errorText));
      return res.status(response.status).json({
        error: `Endpoint returned ${response.status}`,
        details: errorText
      });
    }

    if (isStreaming) {
      // ç›´æ¥è½¬å‘æµå¼å“åº”ï¼Œä¸åšä»»ä½•è½¬æ¢
      res.setHeader('Content-Type', 'text/event-stream');
      res.setHeader('Cache-Control', 'no-cache');
      res.setHeader('Connection', 'keep-alive');

      try {
        // BaSui: æ”¶é›†æµå¼å“åº”ä¸­çš„Tokenç»Ÿè®¡ï¼ˆå®Œæ•´ç‰ˆ - åŒ…å« thinking å’Œ cache tokensï¼‰
        const tokenStats = createTokenStats();
        let buffer = '';

        // ç›´æ¥å°†åŸå§‹å“åº”æµè½¬å‘ç»™å®¢æˆ·ç«¯
        for await (const chunk of response.body) {
          res.write(chunk);

          // BaSui: è§£æSSEæµï¼Œæå–Tokenä½¿ç”¨é‡
          buffer += chunk.toString();
          const lines = buffer.split('\n');
          buffer = lines.pop() || ''; // ä¿ç•™æœ€åä¸å®Œæ•´çš„è¡Œ

          for (const line of lines) {
            if (line.startsWith('data:')) {
              try {
                const data = JSON.parse(line.slice(5).trim());
                // ä½¿ç”¨æ–°çš„ Token æå–å·¥å…·å‡½æ•°ï¼ˆè‡ªåŠ¨å¤„ç†æ‰€æœ‰ Token ç±»å‹ï¼‰
                extractAnthropicTokens(data, tokenStats);
              } catch (e) {
                // å¿½ç•¥éJSONè¡Œ
              }
            }
          }
        }
        res.end();
        logInfo('Stream forwarded successfully');

        // BaSui: è®°å½•Tokenä½¿ç”¨é‡ç»Ÿè®¡ï¼ˆåŒ…å«å®Œæ•´çš„ Token ç±»å‹ï¼‰
        if (tokenStats.inputTokens > 0 || tokenStats.outputTokens > 0) {
          const { recordRequest } = await import('./utils/request-stats.js');
          recordRequest({
            inputTokens: tokenStats.inputTokens,
            outputTokens: tokenStats.outputTokens,
            thinkingTokens: tokenStats.thinkingTokens,
            cacheCreationTokens: tokenStats.cacheCreationTokens,
            cacheReadTokens: tokenStats.cacheReadTokens,
            model: modelId,
            success: true
          });
          logDebug(`æµå¼å“åº”Tokenç»Ÿè®¡(/v1/messages): input=${tokenStats.inputTokens}, output=${tokenStats.outputTokens}, thinking=${tokenStats.thinkingTokens}, cache_creation=${tokenStats.cacheCreationTokens}, cache_read=${tokenStats.cacheReadTokens}`);
        }
      } catch (streamError) {
        logError('Stream error', streamError);
        res.end();
      }
    } else {
      // ç›´æ¥è½¬å‘éæµå¼å“åº”ï¼Œä¸åšä»»ä½•è½¬æ¢
      const data = await response.json();

      // è®°å½•Tokenä½¿ç”¨é‡
      recordTokenUsage(data, 'anthropic', currentKeyId);

      logResponse(200, null, data);
      res.json(data);
    }

  } catch (error) {
    logError('Error in /v1/messages', error);
    // BaSuiï¼šæ£€æŸ¥å“åº”æ˜¯å¦å·²ç»å¼€å§‹å‘é€ï¼Œé¿å…é‡å¤å‘é€å¯¼è‡´å´©æºƒ
    if (!res.headersSent) {
      res.status(500).json({
        error: 'Internal server error',
        message: error.message
      });
    } else {
      // æµå¼å“åº”å·²å¼€å§‹ï¼Œç›´æ¥ç»“æŸè¿æ¥
      res.end();
    }
  }
}

// BaSui: å¤„ç†Anthropic tokenè®¡æ•°è¯·æ±‚ï¼ˆä¸è°ƒç”¨æ¨¡å‹ï¼Œåªè®¡ç®—tokenæ•°ï¼‰
async function handleCountTokens(req, res) {
  logInfo('POST /v1/messages/count_tokens');

  try {
    const anthropicRequest = req.body;
    const modelId = anthropicRequest.model;

    if (!modelId) {
      return res.status(400).json({ error: 'model is required' });
    }

    const model = getModelById(modelId);
    if (!model) {
      return res.status(404).json({ error: `Model ${modelId} not found` });
    }

    // åªå…è®¸ anthropic ç±»å‹ç«¯ç‚¹
    if (model.type !== 'anthropic') {
      return res.status(400).json({
        error: 'Invalid endpoint type',
        message: `/v1/messages/count_tokens æ¥å£åªæ”¯æŒ anthropic ç±»å‹ç«¯ç‚¹ï¼Œå½“å‰æ¨¡å‹ ${modelId} æ˜¯ ${model.type} ç±»å‹`
      });
    }

    const endpoint = getEndpointByType(model.type);
    if (!endpoint) {
      return res.status(500).json({ error: `Endpoint type ${model.type} not found` });
    }

    logInfo(`Counting tokens for ${model.type} endpoint: ${endpoint.base_url}/count_tokens`);

    // Get API key from pool (è½®è¯¢è·å–)
    let authHeader;
    let currentKeyId;
    try {
      const keyResult = await keyPoolManager.getNextKey();
      authHeader = `Bearer ${keyResult.key}`;
      currentKeyId = keyResult.keyId;
    } catch (error) {
      logError('Failed to get API key from pool', error);
      return res.status(500).json({
        error: 'å¯†é’¥æ± é”™è¯¯',
        message: error.message
      });
    }

    const clientHeaders = req.headers;

    // è·å– headersï¼ˆcount_tokensä¸éœ€è¦streamå‚æ•°ï¼‰
    const headers = getAnthropicHeaders(authHeader, clientHeaders, false, modelId);

    logRequest('POST', `${endpoint.base_url}/count_tokens`, headers, anthropicRequest);

    // BaSuiï¼šè°ƒç”¨Anthropicçš„count_tokensç«¯ç‚¹
    const response = await fetchWithPool(`${endpoint.base_url}/count_tokens`, {
      method: 'POST',
      headers,
      body: JSON.stringify(anthropicRequest)
    });

    logInfo(`Response status: ${response.status}`);

    if (!response.ok) {
      const errorText = await response.text();
      logError(`Endpoint error: ${response.status}`, new Error(errorText));
      return res.status(response.status).json({
        error: `Endpoint returned ${response.status}`,
        details: errorText
      });
    }

    // è¿”å›tokenè®¡æ•°ç»“æœ
    const data = await response.json();
    logResponse(200, null, data);
    res.json(data);

  } catch (error) {
    logError('Error in /v1/messages/count_tokens', error);
    res.status(500).json({
      error: 'Internal server error',
      message: error.message
    });
  }
}

// æ³¨å†Œè·¯ç”±
router.post('/v1/chat/completions', handleChatCompletions);
router.post('/v1/responses', handleDirectResponses);
router.post('/v1/messages', handleDirectMessages);
router.post('/v1/messages/count_tokens', handleCountTokens);  // BaSui: æ–°å¢tokenè®¡æ•°ç«¯ç‚¹

export default router;
